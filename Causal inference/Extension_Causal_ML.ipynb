{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extension Causal ML analysis",
   "id": "d904f5d42030e902"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since we got poor results during our initial Causal ML analysis, we tried a different approach to try to ameliorate the score:\n",
    "- bellow you will see updated treatment\n",
    "- Multiple sampling method to reduce class imbalance while keeping integrity of the fraud distribution\n",
    "- X, T and S learner analysis\n",
    "- Shap graph for features importance"
   ],
   "id": "13886c2fa373e64f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Pre-processing",
   "id": "b7e6ba111e5c9673"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-02T20:30:21.620909Z",
     "start_time": "2025-03-02T20:30:16.296926Z"
    }
   },
   "source": [
    "#importing the packages\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T20:30:49.141684Z",
     "start_time": "2025-03-02T20:30:23.935309Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('prepared_data2.csv')",
   "id": "399b236e390125fc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T20:30:50.556386Z",
     "start_time": "2025-03-02T20:30:50.549941Z"
    }
   },
   "cell_type": "code",
   "source": "df.columns",
   "id": "25e703979316f019",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['amount', 'Target', 'current_age', 'per_capita_income', 'yearly_income',\n",
       "       'total_debt', 'credit_score', 'num_credit_cards',\n",
       "       'use_chip_Chip Transaction', 'use_chip_Online Transaction',\n",
       "       'merchant_state_CA', 'merchant_state_FL', 'merchant_state_IL',\n",
       "       'merchant_state_MI', 'merchant_state_NC', 'merchant_state_OH',\n",
       "       'merchant_state_ONLINE', 'merchant_state_Other', 'merchant_state_PA',\n",
       "       'merchant_state_TX', 'gender_male', 'merch_risk_score',\n",
       "       'merch_total_txn', 'avg_txn_merch', 'time_since_last_txn', 'weekend',\n",
       "       'daily_transaction_count', 'weekly_transaction_count',\n",
       "       'amount_change_rate', 'cumulative_amount', 'cumulative_transactions',\n",
       "       'prev_time_since_txn', 'prev_merchant_risk_score',\n",
       "       'txn_time_diff_change', 'merchant_risk_score_change',\n",
       "       'large_amount_change', 'large_txn_time_diff_change',\n",
       "       'large_merchant_risk_change', 'amt_income_ratio', 'debt_income_ratio',\n",
       "       'client_fraud_rate', 'amt_avg_ratio', 'weekday_hour_category_freq',\n",
       "       'typicality_score', 'hour_afternoon', 'hour_evening', 'hour_night',\n",
       "       'time_day_weekday_afternoon', 'time_day_weekday_evening',\n",
       "       'time_day_weekday_morning', 'time_day_weekday_night',\n",
       "       'time_day_weekend_afternoon', 'time_day_weekend_evening',\n",
       "       'time_day_weekend_morning', 'suspicious_indiv_activity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Modeling",
   "id": "f19dce27d7f49cca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Selecting New Treatement that are \"treatable\"\n",
    "\n",
    "We selected these three treatments—high transaction frequency, rapid spending increase, and suspicious activity/merchant risk change—because they capture different dimensions of transactional behavior that can serve as early warning signals for fraud. High transaction frequency is chosen because an unusually high number of transactions in a short period may indicate automated or unauthorized activity, suggesting that interventions such as temporary transaction limits or real-time alerts could be implemented to mitigate risk. Rapid spending increase was selected because a sudden surge in the amount spent, especially when it deviates significantly from a customer’s historical spending patterns, may be indicative of compromised accounts or fraudulent activity; this signal can prompt measures like enhanced verification protocols or spending reviews. Lastly, suspicious activity or significant changes in merchant risk scores are incorporated as they aggregate various qualitative and quantitative factors—from abnormal patterns in merchant behavior to irregularities flagged by anomaly detection systems—thereby offering a broader indicator of risk that can trigger manual investigations or targeted customer outreach. Collectively, these treatments align with our project’s objective of proactively identifying potential fraud, and they represent actionable points where targeted interventions (such as account freezes, transaction alerts, or additional verification steps) can be deployed to reduce financial risk."
   ],
   "id": "83154345994ad549"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Base Model:",
   "id": "64d6e3faec006f07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T03:49:34.496448Z",
     "start_time": "2025-03-03T03:49:27.727888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This script performs causal machine learning analysis using EconML.\n",
    "# It includes:\n",
    "# 1. Data Sampling: A small subset of the data is used to streamline the computation.\n",
    "# 2. Definition of Treatments: Three treatment variables are defined -\n",
    "#    a) High Transaction Frequency\n",
    "#    b) Rapid Spending Increase\n",
    "#    c) Suspicious Activity / Merchant Risk Change\n",
    "# 3. Outcome and Confounders: Specifies the response variable and potential confounders.\n",
    "# 4. Setup of Causal Learners: S-Learner, T-Learner, and X-Learner are applied with Linear Regression and XGBoost as base models.\n",
    "# 5. Estimation of Treatment Effects: Computes ATE (Average Treatment Effect) and CATE (Conditional Average Treatment Effect).\n",
    "#6. We use a sample of the data to allow for testing and it is adjustable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression as LRSRegressor\n",
    "import xgboost as xgb\n",
    "XGBTRegressor = xgb.XGBRegressor\n",
    "from econml.metalearners import SLearner, TLearner, XLearner\n",
    "\n",
    "df_small = df.sample(n=1000, random_state=42)\n",
    "\n",
    "daily_threshold = 10\n",
    "weekly_threshold = 30\n",
    "df_small['treatment_freq'] = ((df_small['daily_transaction_count'] > daily_threshold) |\n",
    "                              (df_small['weekly_transaction_count'] > weekly_threshold)).astype(int)\n",
    "\n",
    "amount_change_threshold = 1.5\n",
    "df_small['treatment_rapid_spend'] = ((df_small['amount_change_rate'] > amount_change_threshold) |\n",
    "                                     (df_small['large_amount_change'] == 1)).astype(int)\n",
    "\n",
    "merchant_risk_threshold = 0.3\n",
    "df_small['treatment_suspicious'] = ((df_small['suspicious_indiv_activity'] == 1) |\n",
    "                                    (df_small['merchant_risk_score_change'] > merchant_risk_threshold)).astype(int)\n",
    "\n",
    "treatment_cols = ['treatment_freq', 'treatment_rapid_spend', 'treatment_suspicious']\n",
    "\n",
    "y = df_small['Target']\n",
    "\n",
    "X = df_small.drop(columns=['Target'] + treatment_cols)\n",
    "\n",
    "learners = {}\n",
    "\n",
    "for treat in treatment_cols:\n",
    "    T = df_small[treat]\n",
    "    learners[treat] = {}\n",
    "\n",
    "    s_lr = SLearner(overall_model=LRSRegressor())\n",
    "    s_lr.fit(y, T, X=X)\n",
    "\n",
    "    s_xgb = SLearner(overall_model=XGBTRegressor(random_state=42))\n",
    "    s_xgb.fit(y, T, X=X)\n",
    "\n",
    "    learners[treat]['S_LRS'] = s_lr\n",
    "    learners[treat]['S_XGBT'] = s_xgb\n",
    "\n",
    "    t_lr = TLearner(models=[LRSRegressor(), LRSRegressor()])\n",
    "    t_lr.fit(y, T, X=X)\n",
    "\n",
    "    t_xgb = TLearner(models=[XGBTRegressor(random_state=42), XGBTRegressor(random_state=42)])\n",
    "    t_xgb.fit(y, T, X=X)\n",
    "\n",
    "    learners[treat]['T_LRS'] = t_lr\n",
    "    learners[treat]['T_XGBT'] = t_xgb\n",
    "\n",
    "    x_lr = XLearner(models=LRSRegressor())\n",
    "    x_lr.fit(y, T, X=X)\n",
    "\n",
    "    x_xgb = XLearner(models=XGBTRegressor(random_state=42))\n",
    "    x_xgb.fit(y, T, X=X)\n",
    "\n",
    "    learners[treat]['X_LRS'] = x_lr\n",
    "    learners[treat]['X_XGBT'] = x_xgb\n",
    "\n",
    "new_X = X.copy()\n",
    "results = {}\n",
    "\n",
    "print(\"Estimated Treatment Effects (ATE and first 5 CATEs):\\n\")\n",
    "for treat in treatment_cols:\n",
    "    results[treat] = {}\n",
    "    print(f\"\\nResults for treatment: {treat}\")\n",
    "    for model_name, model in learners[treat].items():\n",
    "        cate_estimates = model.effect(new_X)\n",
    "        ate_estimate = np.mean(cate_estimates)\n",
    "\n",
    "        results[treat][model_name] = {\n",
    "            'ATE': ate_estimate,\n",
    "            'CATE_first_5': cate_estimates[:5]\n",
    "        }\n",
    "\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  ATE: {ate_estimate:.4f}\")\n",
    "        print(f\"  CATE (first 5): {cate_estimates[:5]}\")"
   ],
   "id": "f62791e8ff18bec4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Treatment Effects (ATE and first 5 CATEs):\n",
      "\n",
      "\n",
      "Results for treatment: treatment_freq\n",
      "S_LRS:\n",
      "  ATE: 0.0089\n",
      "  CATE (first 5): [0.00887915 0.00887915 0.00887915 0.00887915 0.00887915]\n",
      "S_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [0. 0. 0. 0. 0.]\n",
      "T_LRS:\n",
      "  ATE: 0.0020\n",
      "  CATE (first 5): [ 0.01120655 -0.0027439   0.03427999  0.00658482 -0.00534301]\n",
      "T_XGBT:\n",
      "  ATE: 0.0020\n",
      "  CATE (first 5): [ 1.11527061e-05 -8.39112909e-05  1.11527061e-05  1.11527061e-05\n",
      "  9.78377284e-06]\n",
      "X_LRS:\n",
      "  ATE: 0.0071\n",
      "  CATE (first 5): [ 2.17346864e-03 -7.43555790e-05  2.21638762e-02  8.36102527e-03\n",
      "  6.76109304e-04]\n",
      "X_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [1.119645e-05 1.119645e-05 1.119645e-05 1.119645e-05 1.119645e-05]\n",
      "\n",
      "Results for treatment: treatment_rapid_spend\n",
      "S_LRS:\n",
      "  ATE: -0.0053\n",
      "  CATE (first 5): [-0.00534653 -0.00534653 -0.00534653 -0.00534653 -0.00534653]\n",
      "S_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [0. 0. 0. 0. 0.]\n",
      "T_LRS:\n",
      "  ATE: -0.0036\n",
      "  CATE (first 5): [-0.01323379  0.00324641 -0.06916176 -0.00680782  0.01042513]\n",
      "T_XGBT:\n",
      "  ATE: -0.0020\n",
      "  CATE (first 5): [-1.12278149e-05 -1.12377102e-05 -1.12278149e-05 -1.12278149e-05\n",
      " -1.12377102e-05]\n",
      "X_LRS:\n",
      "  ATE: -0.0036\n",
      "  CATE (first 5): [-0.01323379  0.00324641 -0.06916176 -0.00680782  0.01042513]\n",
      "X_XGBT:\n",
      "  ATE: -0.0004\n",
      "  CATE (first 5): [-2.15658376e-05 -1.91390004e-05 -1.97794543e-05 -2.10183839e-05\n",
      " -2.12613216e-05]\n",
      "\n",
      "Results for treatment: treatment_suspicious\n",
      "S_LRS:\n",
      "  ATE: -0.0090\n",
      "  CATE (first 5): [-0.00900784 -0.00900784 -0.00900784 -0.00900784 -0.00900784]\n",
      "S_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [0. 0. 0. 0. 0.]\n",
      "T_LRS:\n",
      "  ATE: 0.1245\n",
      "  CATE (first 5): [0.02795543 0.84034119 0.00911847 0.64638382 0.44135993]\n",
      "T_XGBT:\n",
      "  ATE: 0.0120\n",
      "  CATE (first 5): [1.29599257e-05 1.29546388e-05 1.29599257e-05 1.29599257e-05\n",
      " 1.29546388e-05]\n",
      "X_LRS:\n",
      "  ATE: 0.1241\n",
      "  CATE (first 5): [0.02795543 0.84034119 0.00747922 0.64638382 0.44135993]\n",
      "X_XGBT:\n",
      "  ATE: 0.0129\n",
      "  CATE (first 5): [1.31129346e-05 1.33275534e-05 1.30156183e-05 1.30810040e-05\n",
      " 1.32003924e-05]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Using Standard Scaler  and Logistic Regression",
   "id": "5d4197a894ab9e90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T22:22:55.697744Z",
     "start_time": "2025-03-02T22:22:50.733522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Define Treatment Variables on df_small\n",
    "# - Treatment 1: High Transaction Frequency\n",
    "# - Treatment 2: Rapid Spending Increase\n",
    "# - Treatment 3: Suspicious Activity / Merchant Risk Change\n",
    "# 2. Define Outcome and Confounders, then Scale the Confounders\n",
    "# 3. Set up a logistic regression propensity learner with increased max_iter.\n",
    "# 4. Set Up Causal Learners using EconML with LRSRegressor and XGBTRegressor\n",
    "# 5. Estimate ATE and CATE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression as LRSRegressor\n",
    "import xgboost as xgb\n",
    "XGBTRegressor = xgb.XGBRegressor\n",
    "from econml.metalearners import SLearner, TLearner, XLearner\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df_small = df.sample(n=1000, random_state=42)\n",
    "\n",
    "daily_threshold = 10\n",
    "weekly_threshold = 30\n",
    "df_small['treatment_freq'] = ((df_small['daily_transaction_count'] > daily_threshold) |\n",
    "                              (df_small['weekly_transaction_count'] > weekly_threshold)).astype(int)\n",
    "\n",
    "amount_change_threshold = 1.5\n",
    "df_small['treatment_rapid_spend'] = ((df_small['amount_change_rate'] > amount_change_threshold) |\n",
    "                                     (df_small['large_amount_change'] == 1)).astype(int)\n",
    "\n",
    "merchant_risk_threshold = 0.3\n",
    "df_small['treatment_suspicious'] = ((df_small['suspicious_indiv_activity'] == 1) |\n",
    "                                    (df_small['merchant_risk_score_change'] > merchant_risk_threshold)).astype(int)\n",
    "\n",
    "treatment_cols = ['treatment_freq', 'treatment_rapid_spend', 'treatment_suspicious']\n",
    "\n",
    "y = df_small['Target']\n",
    "\n",
    "X = df_small.drop(columns=['Target'] + treatment_cols)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "propensity_model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "\n",
    "learners = {}\n",
    "\n",
    "for treat in treatment_cols:\n",
    "    T = df_small[treat]\n",
    "    learners[treat] = {}\n",
    "\n",
    "    s_lr = SLearner(overall_model=LRSRegressor())\n",
    "    s_lr.fit(y, T, X=X_scaled)\n",
    "\n",
    "    s_xgb = SLearner(overall_model=XGBTRegressor(random_state=42))\n",
    "    s_xgb.fit(y, T, X=X_scaled)\n",
    "\n",
    "    learners[treat]['S_LRS'] = s_lr\n",
    "    learners[treat]['S_XGBT'] = s_xgb\n",
    "\n",
    "    t_lr = TLearner(models=[LRSRegressor(), LRSRegressor()])\n",
    "    t_lr.fit(y, T, X=X_scaled)\n",
    "\n",
    "    t_xgb = TLearner(models=[XGBTRegressor(random_state=42), XGBTRegressor(random_state=42)])\n",
    "    t_xgb.fit(y, T, X=X_scaled)\n",
    "\n",
    "    learners[treat]['T_LRS'] = t_lr\n",
    "    learners[treat]['T_XGBT'] = t_xgb\n",
    "\n",
    "    x_lr = XLearner(models=LRSRegressor(), propensity_model=propensity_model)\n",
    "    x_lr.fit(y, T, X=X_scaled)\n",
    "\n",
    "    x_xgb = XLearner(models=XGBTRegressor(random_state=42), propensity_model=propensity_model)\n",
    "    x_xgb.fit(y, T, X=X_scaled)\n",
    "\n",
    "    learners[treat]['X_LRS'] = x_lr\n",
    "    learners[treat]['X_XGBT'] = x_xgb\n",
    "\n",
    "new_X = X_scaled.copy()\n",
    "results = {}\n",
    "\n",
    "print(\"Estimated Treatment Effects (ATE and first 5 CATEs):\\n\")\n",
    "for treat in treatment_cols:\n",
    "    results[treat] = {}\n",
    "    print(f\"\\nResults for treatment: {treat}\")\n",
    "    for model_name, model in learners[treat].items():\n",
    "        cate_estimates = model.effect(new_X)\n",
    "        ate_estimate = np.mean(cate_estimates)\n",
    "\n",
    "        results[treat][model_name] = {\n",
    "            'ATE': ate_estimate,\n",
    "            'CATE_first_5': cate_estimates[:5]\n",
    "        }\n",
    "\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  ATE: {ate_estimate:.4f}\")\n",
    "        print(f\"  CATE (first 5): {cate_estimates[:5]}\")"
   ],
   "id": "bf5e7c5bdcd3f62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Treatment Effects (ATE and first 5 CATEs):\n",
      "\n",
      "\n",
      "Results for treatment: treatment_freq\n",
      "S_LRS:\n",
      "  ATE: 0.0042\n",
      "  CATE (first 5): [0.00421143 0.00427246 0.00427246 0.00424194 0.00421143]\n",
      "S_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [0. 0. 0. 0. 0.]\n",
      "T_LRS:\n",
      "  ATE: 0.0022\n",
      "  CATE (first 5): [ 0.01215799 -0.00321731  0.03701279  0.00367966 -0.01145706]\n",
      "T_XGBT:\n",
      "  ATE: 0.0020\n",
      "  CATE (first 5): [ 1.11527061e-05 -8.39112909e-05  1.11527061e-05  1.11527061e-05\n",
      "  9.78377284e-06]\n",
      "X_LRS:\n",
      "  ATE: 0.0094\n",
      "  CATE (first 5): [0.01073324 0.00560032 0.01586046 0.00971487 0.01022724]\n",
      "X_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [1.11964498e-05 1.11913817e-05 1.11964492e-05 1.11964496e-05\n",
      " 1.11962242e-05]\n",
      "\n",
      "Results for treatment: treatment_rapid_spend\n",
      "S_LRS:\n",
      "  ATE: -0.0042\n",
      "  CATE (first 5): [-0.00415039 -0.00415039 -0.00415039 -0.00415039 -0.00415039]\n",
      "S_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [0. 0. 0. 0. 0.]\n",
      "T_LRS:\n",
      "  ATE: -94774798898.7243\n",
      "  CATE (first 5): [ 1.49383545e-02 -6.25610352e-04 -9.87237489e+11 -7.76672363e-03\n",
      " -1.98516846e-02]\n",
      "T_XGBT:\n",
      "  ATE: -0.0020\n",
      "  CATE (first 5): [-1.12278149e-05 -1.12377102e-05 -1.12278149e-05 -1.12278149e-05\n",
      " -1.12377102e-05]\n",
      "X_LRS:\n",
      "  ATE: -94774798898.7227\n",
      "  CATE (first 5): [ 1.04723375e-02  7.35391896e-03 -9.87237489e+11 -1.04967139e-02\n",
      " -1.78567350e-02]\n",
      "X_XGBT:\n",
      "  ATE: -0.0000\n",
      "  CATE (first 5): [-2.22540326e-05 -2.22647741e-05 -1.14859641e-05 -2.22605167e-05\n",
      " -2.22441012e-05]\n",
      "\n",
      "Results for treatment: treatment_suspicious\n",
      "S_LRS:\n",
      "  ATE: -0.0114\n",
      "  CATE (first 5): [-0.01141357 -0.01141357 -0.01139069 -0.01138306 -0.01138306]\n",
      "S_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [0. 0. 0. 0. 0.]\n",
      "T_LRS:\n",
      "  ATE: -9217388078.9150\n",
      "  CATE (first 5): [ 2.90558232e-02  8.35355391e-01 -1.88109961e+11  6.53201144e-01\n",
      "  4.39970451e-01]\n",
      "T_XGBT:\n",
      "  ATE: 0.0120\n",
      "  CATE (first 5): [1.29599257e-05 1.29546388e-05 1.29599257e-05 1.29599257e-05\n",
      " 1.29546388e-05]\n",
      "X_LRS:\n",
      "  ATE: -9217388078.9143\n",
      "  CATE (first 5): [ 2.77629800e-02  8.41660285e-01 -1.88109961e+11  6.50737715e-01\n",
      "  4.39158091e-01]\n",
      "X_XGBT:\n",
      "  ATE: 0.0130\n",
      "  CATE (first 5): [1.29599617e-05 1.29605709e-05 1.54538952e-05 1.29605770e-05\n",
      " 1.29613473e-05]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Using RandomOverSampler",
   "id": "1a1c56fba6ca587e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T22:29:19.886251Z",
     "start_time": "2025-03-02T22:29:14.267801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Treatment variables are defined based on transaction frequency, rapid spending\n",
    "#    increase, and suspicious activity or merchant risk change.\n",
    "# 2. Outcome variable (Target) and confounders are defined. The confounders are scaled\n",
    "#    using `StandardScaler`.\n",
    "# 3. Multiple causal learners (S-Learner, T-Learner, and X-Learner) are trained for\n",
    "#    each treatment variable after addressing class imbalance using `RandomOverSampler`.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression as LRSRegressor\n",
    "import xgboost as xgb\n",
    "XGBTRegressor = xgb.XGBRegressor\n",
    "from econml.metalearners import SLearner, TLearner, XLearner\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "df_small = df.sample(n=1000, random_state=42)\n",
    "daily_threshold = 10\n",
    "weekly_threshold = 30\n",
    "df_small['treatment_freq'] = ((df_small['daily_transaction_count'] > daily_threshold) |\n",
    "                              (df_small['weekly_transaction_count'] > weekly_threshold)).astype(int)\n",
    "amount_change_threshold = 1.5\n",
    "df_small['treatment_rapid_spend'] = ((df_small['amount_change_rate'] > amount_change_threshold) |\n",
    "                                     (df_small['large_amount_change'] == 1)).astype(int)\n",
    "merchant_risk_threshold = 0.3\n",
    "df_small['treatment_suspicious'] = ((df_small['suspicious_indiv_activity'] == 1) |\n",
    "                                    (df_small['merchant_risk_score_change'] > merchant_risk_threshold)).astype(int)\n",
    "treatment_cols = ['treatment_freq', 'treatment_rapid_spend', 'treatment_suspicious']\n",
    "y = df_small['Target']\n",
    "X = df_small.drop(columns=['Target'] + treatment_cols)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "propensity_model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "learners = {}\n",
    "results = {}\n",
    "\n",
    "for treat in treatment_cols:\n",
    "    T = df_small[treat]\n",
    "    df_temp = X_scaled.copy()\n",
    "    df_temp['T'] = T.values\n",
    "    df_temp['y'] = y.values\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    df_resampled, _ = ros.fit_resample(df_temp, df_temp['T'])\n",
    "    X_res = df_resampled.drop(columns=['T', 'y'])\n",
    "    T_res = df_resampled['T']\n",
    "    y_res = df_resampled['y']\n",
    "    learners[treat] = {}\n",
    "    s_lr = SLearner(overall_model=LRSRegressor())\n",
    "    s_lr.fit(y_res, T_res, X=X_res)\n",
    "    s_xgb = SLearner(overall_model=XGBTRegressor(random_state=42))\n",
    "    s_xgb.fit(y_res, T_res, X=X_res)\n",
    "    learners[treat]['S_LRS'] = s_lr\n",
    "    learners[treat]['S_XGBT'] = s_xgb\n",
    "    t_lr = TLearner(models=[LRSRegressor(), LRSRegressor()])\n",
    "    t_lr.fit(y_res, T_res, X=X_res)\n",
    "    t_xgb = TLearner(models=[XGBTRegressor(random_state=42), XGBTRegressor(random_state=42)])\n",
    "    t_xgb.fit(y_res, T_res, X=X_res)\n",
    "    learners[treat]['T_LRS'] = t_lr\n",
    "    learners[treat]['T_XGBT'] = t_xgb\n",
    "    x_lr = XLearner(models=LRSRegressor(), propensity_model=propensity_model)\n",
    "    x_lr.fit(y_res, T_res, X=X_res)\n",
    "    x_xgb = XLearner(models=XGBTRegressor(random_state=42), propensity_model=propensity_model)\n",
    "    x_xgb.fit(y_res, T_res, X=X_res)\n",
    "    learners[treat]['X_LRS'] = x_lr\n",
    "    learners[treat]['X_XGBT'] = x_xgb\n",
    "    new_X = X_res.copy()\n",
    "    results[treat] = {}\n",
    "    print(f\"\\nResults for treatment: {treat}\")\n",
    "    for model_name, model in learners[treat].items():\n",
    "        cate_estimates = model.effect(new_X)\n",
    "        ate_estimate = np.mean(cate_estimates)\n",
    "        results[treat][model_name] = {\n",
    "            'ATE': ate_estimate,\n",
    "            'CATE_first_5': cate_estimates[:5]\n",
    "        }\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  ATE: {ate_estimate:.4f}\")\n",
    "        print(f\"  CATE (first 5): {cate_estimates[:5]}\")"
   ],
   "id": "32e82571bc2c0036",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for treatment: treatment_freq\n",
      "S_LRS:\n",
      "  ATE: 0.0073\n",
      "  CATE (first 5): [0.00730896 0.00726318 0.0072937  0.00730133 0.00730133]\n",
      "S_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [0. 0. 0. 0. 0.]\n",
      "T_LRS:\n",
      "  ATE: 0.0067\n",
      "  CATE (first 5): [ 0.01215799 -0.00321731  0.03701279  0.00367966 -0.01145706]\n",
      "T_XGBT:\n",
      "  ATE: 0.0010\n",
      "  CATE (first 5): [ 1.11527061e-05 -8.39112909e-05  1.11527061e-05  1.11527061e-05\n",
      "  9.78377284e-06]\n",
      "X_LRS:\n",
      "  ATE: 0.0090\n",
      "  CATE (first 5): [ 0.00601891 -0.00110475 -0.0007485   0.00646459  0.01338162]\n",
      "X_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [1.11994332e-05 1.11991843e-05 1.11994332e-05 1.11994332e-05\n",
      " 1.11994155e-05]\n",
      "\n",
      "Results for treatment: treatment_rapid_spend\n",
      "S_LRS:\n",
      "  ATE: -0.0050\n",
      "  CATE (first 5): [-0.00498199 -0.00497437 -0.00500488 -0.00498962 -0.00497437]\n",
      "S_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [0. 0. 0. 0. 0.]\n",
      "T_LRS:\n",
      "  ATE: -302407669310.8331\n",
      "  CATE (first 5): [ 1.49383545e-02 -6.25610352e-04 -9.87237489e+11 -7.76672363e-03\n",
      " -1.98516846e-02]\n",
      "T_XGBT:\n",
      "  ATE: -0.0012\n",
      "  CATE (first 5): [-1.12278149e-05 -1.12377102e-05 -1.12278149e-05 -1.12278149e-05\n",
      " -1.12377102e-05]\n",
      "X_LRS:\n",
      "  ATE: -302407669310.8320\n",
      "  CATE (first 5): [ 1.10355864e-02  5.49298348e-03 -9.87237489e+11 -9.82595766e-03\n",
      " -1.59937547e-02]\n",
      "X_XGBT:\n",
      "  ATE: -0.0000\n",
      "  CATE (first 5): [-1.65776256e-05 -1.65822408e-05 -1.12521755e-05 -1.65805725e-05\n",
      " -1.65788768e-05]\n",
      "\n",
      "Results for treatment: treatment_suspicious\n",
      "S_LRS:\n",
      "  ATE: -0.0080\n",
      "  CATE (first 5): [-0.0078125  -0.00793457 -0.00793457 -0.00805664 -0.00805664]\n",
      "S_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [0. 0. 0. 0. 0.]\n",
      "T_LRS:\n",
      "  ATE: -99404914473.6506\n",
      "  CATE (first 5): [ 9.94300842e-03  8.30472946e-01 -1.88109961e+11  6.49061203e-01\n",
      "  4.40076828e-01]\n",
      "T_XGBT:\n",
      "  ATE: 0.0179\n",
      "  CATE (first 5): [-6.10200368e-06 -6.10729057e-06 -6.10200368e-06 -6.10200368e-06\n",
      " -6.10729057e-06]\n",
      "X_LRS:\n",
      "  ATE: -114307006939.1225\n",
      "  CATE (first 5): [ 9.60476534e-03  8.38247378e-01 -1.96671711e+11  6.48398672e-01\n",
      "  4.39689531e-01]\n",
      "X_XGBT:\n",
      "  ATE: 0.0184\n",
      "  CATE (first 5): [-6.10194901e-06 -6.10119138e-06 -3.57261286e-06 -6.10113330e-06\n",
      " -6.09940794e-06]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### USING ADASYN and Ridge Regressor",
   "id": "95e84f4e5cfa76a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T22:40:15.591360Z",
     "start_time": "2025-03-02T22:38:12.044505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Preparing data for modeling (scaling, creating treatments, and resampling imbalanced data).\n",
    "# 2. Training meta-learners for each treatment using different models (Ridge Regressor and XGBoost Regressor).\n",
    "# 3. Evaluating the average treatment effect (ATE) and conditional average treatment effects (CATEs).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from econml.metalearners import SLearner, TLearner, XLearner\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "RRSRegressor = Ridge()\n",
    "XGBTRegressor = XGBRegressor\n",
    "\n",
    "df_small = df.sample(n=600000, random_state=42)\n",
    "\n",
    "daily_threshold = 10\n",
    "weekly_threshold = 30\n",
    "amount_change_threshold = 1.5\n",
    "merchant_risk_threshold = 0.3\n",
    "\n",
    "df_small['treatment_freq'] = ((df_small['daily_transaction_count'] > daily_threshold) |\n",
    "                              (df_small['weekly_transaction_count'] > weekly_threshold)).astype(int)\n",
    "\n",
    "df_small['treatment_rapid_spend'] = ((df_small['amount_change_rate'] > amount_change_threshold) |\n",
    "                                     (df_small['large_amount_change'] == 1)).astype(int)\n",
    "\n",
    "df_small['treatment_suspicious'] = ((df_small['suspicious_indiv_activity'] == 1) |\n",
    "                                    (df_small['merchant_risk_score_change'] > merchant_risk_threshold)).astype(int)\n",
    "\n",
    "treatment_cols = ['treatment_freq', 'treatment_rapid_spend', 'treatment_suspicious']\n",
    "y = df_small['Target']\n",
    "X = df_small.drop(columns=['Target'] + treatment_cols)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "propensity_model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "\n",
    "learners = {}\n",
    "results = {}\n",
    "\n",
    "for treat in treatment_cols:\n",
    "    T = df_small[treat]\n",
    "    df_temp = X_scaled.copy()\n",
    "    df_temp['T'] = T.values\n",
    "    df_temp['y'] = y.values\n",
    "\n",
    "    adasyn = ADASYN(random_state=42, n_neighbors=min(5, df_temp['T'].value_counts().min() - 1))\n",
    "    df_resampled, _ = adasyn.fit_resample(df_temp, df_temp['T'])\n",
    "\n",
    "    X_res = df_resampled.drop(columns=['T', 'y'])\n",
    "    T_res = df_resampled['T']\n",
    "    y_res = df_resampled['y']\n",
    "\n",
    "    learners[treat] = {}\n",
    "\n",
    "    s_rrs = SLearner(overall_model=RRSRegressor)\n",
    "    s_rrs.fit(y_res, T_res, X=X_res)\n",
    "    learners[treat]['S_RRS'] = s_rrs\n",
    "\n",
    "    s_xgb = SLearner(overall_model=XGBTRegressor(random_state=42))\n",
    "    s_xgb.fit(y_res, T_res, X=X_res)\n",
    "    learners[treat]['S_XGBT'] = s_xgb\n",
    "\n",
    "    t_rrs = TLearner(models=[RRSRegressor, RRSRegressor])\n",
    "    t_rrs.fit(y_res, T_res, X=X_res)\n",
    "    learners[treat]['T_RRS'] = t_rrs\n",
    "\n",
    "    t_xgb = TLearner(models=[XGBTRegressor(random_state=42), XGBTRegressor(random_state=42)])\n",
    "    t_xgb.fit(y_res, T_res, X=X_res)\n",
    "    learners[treat]['T_XGBT'] = t_xgb\n",
    "\n",
    "    x_rrs = XLearner(models=RRSRegressor, propensity_model=propensity_model)\n",
    "    x_rrs.fit(y_res, T_res, X=X_res)\n",
    "    learners[treat]['X_RRS'] = x_rrs\n",
    "\n",
    "    x_xgb = XLearner(models=XGBTRegressor(random_state=42), propensity_model=propensity_model)\n",
    "    x_xgb.fit(y_res, T_res, X=X_res)\n",
    "    learners[treat]['X_XGBT'] = x_xgb\n",
    "\n",
    "    new_X = X_res.copy()\n",
    "    results[treat] = {}\n",
    "\n",
    "    for model_name, model in learners[treat].items():\n",
    "        cate_estimates = model.effect(new_X)\n",
    "        ate_estimate = np.mean(cate_estimates)\n",
    "\n",
    "        results[treat][model_name] = {\n",
    "            'ATE': ate_estimate,\n",
    "            'CATE_first_5': cate_estimates[:5]\n",
    "        }\n",
    "\n",
    "        print(f\"\\nResults for treatment: {treat}\")\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  ATE: {ate_estimate:.4f}\")\n",
    "        print(f\"  CATE (first 5): {cate_estimates[:5]}\")"
   ],
   "id": "f3d2ad26e9cc2b54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for treatment: treatment_freq\n",
      "S_RRS:\n",
      "  ATE: 0.0032\n",
      "  CATE (first 5): [0.00321089 0.00321089 0.00321089 0.00321089 0.00321089]\n",
      "\n",
      "Results for treatment: treatment_freq\n",
      "S_XGBT:\n",
      "  ATE: 0.0009\n",
      "  CATE (first 5): [3.49410716e-03 2.62449525e-04 1.15286303e-03 5.78663545e-04\n",
      " 6.21874206e-05]\n",
      "\n",
      "Results for treatment: treatment_freq\n",
      "T_RRS:\n",
      "  ATE: 0.0030\n",
      "  CATE (first 5): [0.00510554 0.00137916 0.00990803 0.00028266 0.00332597]\n",
      "\n",
      "Results for treatment: treatment_freq\n",
      "T_XGBT:\n",
      "  ATE: -0.0002\n",
      "  CATE (first 5): [-9.50012065e-04 -7.45160069e-05 -2.96447705e-03  4.73147666e-04\n",
      " -8.01460701e-05]\n",
      "\n",
      "Results for treatment: treatment_freq\n",
      "X_RRS:\n",
      "  ATE: 0.0030\n",
      "  CATE (first 5): [0.00510802 0.00137468 0.00990731 0.00027842 0.00332205]\n",
      "\n",
      "Results for treatment: treatment_freq\n",
      "X_XGBT:\n",
      "  ATE: 0.0056\n",
      "  CATE (first 5): [ 2.80486979e-03 -8.01083893e-02 -7.99820991e-04 -5.77082865e-05\n",
      "  6.94155227e-03]\n",
      "\n",
      "Results for treatment: treatment_rapid_spend\n",
      "S_RRS:\n",
      "  ATE: -0.0008\n",
      "  CATE (first 5): [-0.0007807 -0.0007807 -0.0007807 -0.0007807 -0.0007807]\n",
      "\n",
      "Results for treatment: treatment_rapid_spend\n",
      "S_XGBT:\n",
      "  ATE: -0.0002\n",
      "  CATE (first 5): [1.93638261e-05 2.47881981e-05 1.93638261e-05 2.47880816e-05\n",
      " 1.93638698e-05]\n",
      "\n",
      "Results for treatment: treatment_rapid_spend\n",
      "T_RRS:\n",
      "  ATE: -0.0006\n",
      "  CATE (first 5): [-0.00124954 -0.00115857 -0.00407063  0.0001579  -0.00194665]\n",
      "\n",
      "Results for treatment: treatment_rapid_spend\n",
      "T_XGBT:\n",
      "  ATE: -0.0018\n",
      "  CATE (first 5): [-0.00368321 -0.00082362 -0.00068417 -0.00080886  0.00017802]\n",
      "\n",
      "Results for treatment: treatment_rapid_spend\n",
      "X_RRS:\n",
      "  ATE: -0.0007\n",
      "  CATE (first 5): [-0.00125429 -0.00115568 -0.00574162  0.00015826 -0.0019446 ]\n",
      "\n",
      "Results for treatment: treatment_rapid_spend\n",
      "X_XGBT:\n",
      "  ATE: -0.0031\n",
      "  CATE (first 5): [-0.00278251 -0.00225522 -0.00385095  0.00019279 -0.00104134]\n",
      "\n",
      "Results for treatment: treatment_suspicious\n",
      "S_RRS:\n",
      "  ATE: -0.0043\n",
      "  CATE (first 5): [-0.00429339 -0.00429339 -0.00429339 -0.00429339 -0.00429339]\n",
      "\n",
      "Results for treatment: treatment_suspicious\n",
      "S_XGBT:\n",
      "  ATE: 0.0000\n",
      "  CATE (first 5): [-2.83365458e-04 -2.83365778e-04  0.00000000e+00 -2.83365691e-04\n",
      " -5.59672335e-05]\n",
      "\n",
      "Results for treatment: treatment_suspicious\n",
      "T_RRS:\n",
      "  ATE: -0.0088\n",
      "  CATE (first 5): [-0.00977594 -0.01946911  0.00196013 -0.01598963 -0.01829916]\n",
      "\n",
      "Results for treatment: treatment_suspicious\n",
      "T_XGBT:\n",
      "  ATE: 0.0301\n",
      "  CATE (first 5): [0.11056279 0.06692398 0.00063308 0.06655634 0.06608507]\n",
      "\n",
      "Results for treatment: treatment_suspicious\n",
      "X_RRS:\n",
      "  ATE: -0.0169\n",
      "  CATE (first 5): [-0.00978621 -0.01946732 -0.01451609 -0.01598793 -0.01829672]\n",
      "\n",
      "Results for treatment: treatment_suspicious\n",
      "X_XGBT:\n",
      "  ATE: 0.0340\n",
      "  CATE (first 5): [ 0.14076731 -0.00373836  0.0796249  -0.06759777 -0.0506126 ]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### USING ROS, Hypermarameter tunning",
   "id": "65f6b17d5799b819"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T04:05:36.566161Z",
     "start_time": "2025-03-03T04:01:37.994857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Sample a subset of the dataset for faster runtime during experimentation.\n",
    "# 2. Define treatment variables based on transactional behavior thresholds.\n",
    "# 3. Define the outcome ('Target') and apply scaling to the features (confounders).\n",
    "# 4. Perform hyperparameter tuning to find the best parameters for Ridge regression\n",
    "#    and XGBoost models, which are used as base learners.\n",
    "# 5. Use `RandomOverSampler` to balance treatment groups, ensuring no class imbalance\n",
    "#    while fitting the models.\n",
    "# 6. Train S-Learner, T-Learner, and X-Learner models for each defined treatment.\n",
    "# 7. Estimate both the Average Treatment Effect (ATE) and Conditional Average Treatment Effects (CATE).\n",
    "#    CATE reflects estimates for specific subsets of the data.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from econml.metalearners import SLearner, TLearner, XLearner\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "df_small = df.sample(n=600000, random_state=42)\n",
    "\n",
    "daily_threshold = 10\n",
    "weekly_threshold = 30\n",
    "df_small['treatment_freq'] = ((df_small['daily_transaction_count'] > daily_threshold) |\n",
    "                              (df_small['weekly_transaction_count'] > weekly_threshold)).astype(int)\n",
    "\n",
    "amount_change_threshold = 1.5\n",
    "df_small['treatment_rapid_spend'] = ((df_small['amount_change_rate'] > amount_change_threshold) |\n",
    "                                     (df_small['large_amount_change'] == 1)).astype(int)\n",
    "\n",
    "merchant_risk_threshold = 0.3\n",
    "df_small['treatment_suspicious'] = ((df_small['suspicious_indiv_activity'] == 1) |\n",
    "                                    (df_small['merchant_risk_score_change'] > merchant_risk_threshold)).astype(int)\n",
    "\n",
    "treatment_cols = ['treatment_freq', 'treatment_rapid_spend', 'treatment_suspicious']\n",
    "\n",
    "y = df_small['Target']\n",
    "X = df_small.drop(columns=['Target'] + treatment_cols)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "propensity_model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "\n",
    "param_grid_ridge = {'alpha': [0.1, 1.0, 10.0, 100.0]}\n",
    "grid_ridge = GridSearchCV(Ridge(), param_grid_ridge, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_ridge.fit(X_scaled, y)\n",
    "best_alpha = grid_ridge.best_params_['alpha']\n",
    "print(\"Best Ridge alpha:\", best_alpha)\n",
    "\n",
    "param_grid_xgb = {'n_estimators': [50, 100, 200],\n",
    "                  'max_depth': [3, 5, 7],\n",
    "                  'learning_rate': [0.01, 0.1, 0.2]}\n",
    "grid_xgb = GridSearchCV(XGBRegressor(random_state=42, objective='reg:squarederror'),\n",
    "                        param_grid_xgb, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_xgb.fit(X_scaled, y)\n",
    "best_params_xgb = grid_xgb.best_params_\n",
    "print(\"Best XGBoost params:\", best_params_xgb)\n",
    "\n",
    "def get_tuned_ridge():\n",
    "    return Ridge(alpha=best_alpha)\n",
    "\n",
    "def get_tuned_xgb():\n",
    "    return XGBRegressor(random_state=42, objective='reg:squarederror', **best_params_xgb)\n",
    "\n",
    "learners = {}\n",
    "results = {}\n",
    "\n",
    "for treat in treatment_cols:\n",
    "    T = df_small[treat]\n",
    "    df_temp = X_scaled.copy()\n",
    "    df_temp['T'] = T.values\n",
    "    df_temp['y'] = y.values\n",
    "\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    df_resampled, _ = ros.fit_resample(df_temp, df_temp['T'])\n",
    "\n",
    "    X_res = df_resampled.drop(columns=['T', 'y'])\n",
    "    T_res = df_resampled['T']\n",
    "    y_res = df_resampled['y']\n",
    "\n",
    "    learners[treat] = {}\n",
    "\n",
    "    s_lr = SLearner(overall_model=get_tuned_ridge())\n",
    "    s_lr.fit(y_res, T_res, X=X_res)\n",
    "\n",
    "    s_xgb = SLearner(overall_model=get_tuned_xgb())\n",
    "    s_xgb.fit(y_res, T_res, X=X_res)\n",
    "\n",
    "    learners[treat]['S_Ridge'] = s_lr\n",
    "    learners[treat]['S_XGBT'] = s_xgb\n",
    "\n",
    "    t_lr = TLearner(models=[get_tuned_ridge(), get_tuned_ridge()])\n",
    "    t_lr.fit(y_res, T_res, X=X_res)\n",
    "\n",
    "    t_xgb = TLearner(models=[get_tuned_xgb(), get_tuned_xgb()])\n",
    "    t_xgb.fit(y_res, T_res, X=X_res)\n",
    "\n",
    "    learners[treat]['T_Ridge'] = t_lr\n",
    "    learners[treat]['T_XGBT'] = t_xgb\n",
    "\n",
    "    x_lr = XLearner(models=get_tuned_ridge(), propensity_model=propensity_model)\n",
    "    x_lr.fit(y_res, T_res, X=X_res)\n",
    "\n",
    "    x_xgb = XLearner(models=get_tuned_xgb(), propensity_model=propensity_model)\n",
    "    x_xgb.fit(y_res, T_res, X=X_res)\n",
    "\n",
    "    learners[treat]['X_LRS'] = x_lr\n",
    "    learners[treat]['X_XGBT'] = x_xgb\n",
    "\n",
    "    new_X = X_res.copy()\n",
    "    results[treat] = {}\n",
    "    print(f\"\\nResults for treatment: {treat}\")\n",
    "    for model_name, model in learners[treat].items():\n",
    "        cate_estimates = model.effect(new_X)\n",
    "        ate_estimate = np.mean(cate_estimates)\n",
    "        results[treat][model_name] = {'ATE': ate_estimate, 'CATE_first_5': cate_estimates[:5]}\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  ATE: {ate_estimate:.4f}\")\n",
    "        print(f\"  CATE (first 5): {cate_estimates[:5]}\")"
   ],
   "id": "c77011fe0a7f45c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Ridge alpha: 0.1\n",
      "Best XGBoost params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "\n",
      "Results for treatment: treatment_freq\n",
      "S_Ridge:\n",
      "  ATE: 0.0012\n",
      "  CATE (first 5): [0.00123708 0.00123708 0.00123708 0.00123708 0.00123708]\n",
      "S_XGBT:\n",
      "  ATE: -0.0000\n",
      "  CATE (first 5): [0. 0. 0. 0. 0.]\n",
      "T_Ridge:\n",
      "  ATE: 0.0299\n",
      "  CATE (first 5): [0.0726579  0.02063734 0.03872644 0.10074011 0.03218214]\n",
      "T_XGBT:\n",
      "  ATE: -0.0020\n",
      "  CATE (first 5): [ 3.10726394e-03 -2.06862052e-04  1.70826376e-03  6.52611707e-05\n",
      " -1.64885877e-03]\n",
      "X_LRS:\n",
      "  ATE: 0.0299\n",
      "  CATE (first 5): [0.07265799 0.02063695 0.03872632 0.10073951 0.03218173]\n",
      "X_XGBT:\n",
      "  ATE: -0.0021\n",
      "  CATE (first 5): [0.00468646 0.00539277 0.00157829 0.00166613 0.0022906 ]\n",
      "\n",
      "Results for treatment: treatment_rapid_spend\n",
      "S_Ridge:\n",
      "  ATE: 0.0009\n",
      "  CATE (first 5): [0.0008625 0.0008625 0.0008625 0.0008625 0.0008625]\n",
      "S_XGBT:\n",
      "  ATE: -0.0000\n",
      "  CATE (first 5): [3.85283493e-05 4.25242979e-05 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00]\n",
      "T_Ridge:\n",
      "  ATE: 0.0013\n",
      "  CATE (first 5): [0.01240488 0.00045693 0.0009745  0.00155301 0.00112752]\n",
      "T_XGBT:\n",
      "  ATE: 0.0010\n",
      "  CATE (first 5): [-0.00611229  0.00180527 -0.0007044  -0.00052202  0.00087651]\n",
      "X_LRS:\n",
      "  ATE: 0.0007\n",
      "  CATE (first 5): [ 0.01240444  0.00045716 -0.0008178   0.00155305  0.00112772]\n",
      "X_XGBT:\n",
      "  ATE: 0.0015\n",
      "  CATE (first 5): [-2.78031291e-03  2.06363839e-05  1.74897747e-03  6.17570651e-04\n",
      "  1.45659540e-02]\n",
      "\n",
      "Results for treatment: treatment_suspicious\n",
      "S_Ridge:\n",
      "  ATE: 0.0186\n",
      "  CATE (first 5): [0.01857338 0.01857338 0.01857338 0.01857338 0.01857338]\n",
      "S_XGBT:\n",
      "  ATE: 0.0001\n",
      "  CATE (first 5): [ 4.71389294e-03  1.99319649e-04  1.88023754e-04 -3.58241668e-05\n",
      " -1.44878868e-06]\n",
      "T_Ridge:\n",
      "  ATE: 0.0107\n",
      "  CATE (first 5): [0.05829431 0.00733083 0.01586347 0.01644175 0.01670767]\n",
      "T_XGBT:\n",
      "  ATE: 0.0001\n",
      "  CATE (first 5): [ 0.00247288 -0.00075934  0.00201477  0.00051509 -0.00050762]\n",
      "X_LRS:\n",
      "  ATE: 0.0170\n",
      "  CATE (first 5): [0.05829326 0.00733092 0.02873763 0.01644185 0.01670782]\n",
      "X_XGBT:\n",
      "  ATE: 0.0574\n",
      "  CATE (first 5): [ 0.55361567  0.36824535  0.0040246  -0.23748708 -0.23043488]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Interpretation\n",
    "\n",
    "Treatment: High Transaction Frequency: The estimated effects for high transaction frequency vary across models. The Ridge‐based S‑Learner indicates a very small positive effect (an average treatment effect, or ATE, of about 0.12%), suggesting that an increase in transaction frequency might be weakly associated with a slightly higher risk or outcome value. However, when using the T‑Learner and X‑Learner with Ridge, the ATE jumps to roughly 3%, indicating that under these modeling frameworks, high frequency appears to be more influential—but this is not corroborated by the XGBoost‑based models, which consistently report negligible or even slightly negative effects.\n",
    "\n",
    "Treatment: Rapid Spending Increase: For rapid spending increase, the linear models (across S‑, T‑, and X‑Learners using Ridge) show very small positive effects (with ATEs in the range of about 0.07% to 0.13%), whereas the XGBoost‑based models again indicate nearly zero impact. The minimal effect sizes across all specifications imply that a sudden surge in spending is not, by itself, a strong driver of the target outcome. From a business perspective, this weak signal means that relying solely on rapid spending as an indicator may not be sufficient to justify strong remedial actions.\n",
    "\n",
    "Treatment: Suspicious Activity / Merchant Risk Change: The results for suspicious activity exhibit the most significant variation. The linear models (S‑Learner and X‑Learner with Ridge) suggest a meaningful positive effect, with ATEs ranging from roughly 1.7% to 1.8%, and the T‑Learner indicating an even higher average effect of around 1%. More notably, one of the XGBoost‑based learners (the X‑Learner) estimates a considerably higher average effect (about 5.7%), with individual conditional effects (CATEs) reaching as high as 55% for some observations. This wide range points to substantial heterogeneity—indicating that while a subset of transactions flagged as suspicious shows a strong association with increased risk, others do not. This variability is critical from a business standpoint: it suggests that suspicious activity is a potentially powerful indicator, but its signal is inconsistent across the population."
   ],
   "id": "26c7a46e0c681026"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
